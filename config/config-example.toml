[app]
    segment_duration = 5 # 音频切分处理间隔，单位：分钟，建议值：5-10，如果视频中话语较少可以适当提高
    transcribe_parallel_num = 5 # 并发进行转录的数量上限，建议值：5，如果使用了本地模型，可以适当调低
    translate_parallel_num = 10 # 并发进行翻译的数量上限，建议值：2倍于转录的并发量，如果使用TPM限制严格的API，可以适当调低
    transcribe_max_attempts = 3 # 转录最大尝试次数，建议值：3
    translate_max_attempts = 5 # 翻译最大尝试次数，建议值：5，如果模型参数量较少或翻译失败率较高可以适当调高
    proxy = "" # 网络代理地址，格式如http://127.0.0.1:7890，可不填
    transcribe_provider = "openai" # 语音识别，当前可选值：openai,fasterwhisper,whisperkit,whisper.cpp,aliyun。(fasterwhisper不支持macOS,whisperkit只支持M芯片)
    llm_provider = "openai" # LLM，当前可选值：openai,aliyun

[server]
    host = "127.0.0.1"
    port = 8888

# 下方的配置非必填，请结合上方的选项和文档说明进行配置
[local_model]
    fasterwhisper = "large-v2" # fasterwhisper的本地模型可选值：tiny,medium,large-v2，建议medium及以上
    whisperkit = "large-v2" # whisperkit的本地模型可选值：large-v2
    whispercpp = "large-v2" # whisper.cpp的本地模型

[openai]
    base_url = "" # OpenAI API 自定义base url，可配合转发站密钥使用，留空为默认API地址
    model = "" # 指定模型名，可通过此字段结合base_url使用外部任何与OpenAI API兼容的大模型服务，留空默认为gpt-4o-mini
    api_key = "sk-XXX" # OpenAI API密钥
    [openai.whisper] # 由于使用whisperAPI进行语音识别时，上方可能配置使用了OpenAI格式兼容的其它厂商的模型，所以此处需要独立填入openai的配置信息
        base_url = ""
        api_key = ""

[aliyun] # 具体请参考文档中的“阿里云配置说明”
    [aliyun.oss]
        access_key_id = ""
        access_key_secret = ""
        bucket = ""
    [aliyun.speech]
        access_key_id = ""
        access_key_secret = ""
        app_key= ""
    [aliyun.bailian]
        api_key = ""